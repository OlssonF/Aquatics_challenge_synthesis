---
title: "figures?"
output: html_document
date: "2023-11-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(arrow)
library(lubridate)
library(tidytext)
library(ggpattern)
```

## Read in the data and model metadata

```{r read-in-data}

lake_sites <- c('BARC', 'CRAM', 'LIRO', 'PRLA', 'PRPO', 'SUGG', 'TOOK')
cols_modeltype <- c('empirical' = "#482576FF",
          'multi-model ensemble' =  "#35608DFF",
          'process' = "#21908CFF",
          'ML' = '#BBDF27FF',
          'null' = '#43BF71FF')
# read in the metadata from the google sheet
googlesheets4::gs4_deauth()
model_meta <- googlesheets4::read_sheet("https://docs.google.com/spreadsheets/d/1oC7_w63wSCXNiHs1IK8AFGr0MG-NdjDAjwkfjvRZW-I/edit?usp=sharing")

# Open dataset-------------
# start with lake temperatures
temperature_scores <- arrow::open_dataset("../scores_rescored") |>
  filter(site_id %in% lake_sites,
         # model_id != 'climatology',
         model_id %in% model_meta$model_id,
         variable == 'temperature') |>
  collect()


# Calculate skill scores ------------
temperature_climatology <- arrow::open_dataset("../scores_rescored") |>
  filter(model_id == 'climatology', 
         site_id %in% lake_sites,
         variable == 'temperature') |>
  collect() |> 
  select(reference_datetime, datetime, crps, logs, site_id, model_id) |> 
  arrange(reference_datetime, datetime, site_id) |> 
  pivot_wider(values_from = c(crps, logs),
              names_from = model_id)

temperature_skill <- temperature_scores |>
  filter(model_id != 'climatology') |> 
  select(reference_datetime, datetime, crps, logs, site_id, model_id) |> 
  full_join(temperature_climatology) |> 
  
  # calcaulte the skill relative to climatology (- is bad, + is good)
  mutate(skill_crps = crps_climatology - crps,
         skill_logs = logs_climatology - logs,
         horizon = as.numeric(as_date(datetime) - as_date(reference_datetime))) |> 
  # consistent forecast period
  filter(horizon <= 30,
         horizon >= 1,
         model_id != 'fARIMA') |> 
  arrange(reference_datetime, site_id, datetime, model_id) 
```

## Figure 1

Top 10 models and all models coloured by type and showing if they include covariates or not. 
```{r model-type, fig.width=10}
top10_temp <- 
  temperature_skill |> 
  group_by(model_id) |> 
  summarise(mean_crps = mean(skill_crps, na.rm = T)) |> 
  ungroup() |>
  slice_max(mean_crps, n= 10, na_rm = T) |>
  left_join(model_meta) |> 
  mutate(model_id = fct_reorder(model_id, mean_crps)) |> 
  ggplot(aes(x=mean_crps, y = model_id, fill = model_type)) +    # Different pattern for each group
  geom_bar_pattern(aes(pattern = as.factor(uses_NOAA)),
                   stat = "identity",
                   alpha = 0.8, 
                   pattern_fill = "black",
                   colour = "black", 
                   pattern_spacing = 0.02,
                   pattern_alpha = 0.5) + 
  scale_y_reordered() +
  labs(y='Model ID', x = 'mean CRPS skill (vs climatology)') +
  geom_vline(xintercept = 0) +
  scale_fill_manual(values = cols_modeltype) +
  scale_pattern_manual(values=c('none', 'stripe')) +
  scale_pattern_type_manual(values=c(NA, NA)) +
  theme_bw()

all_temperature <- temperature_skill |> 
  group_by(model_id) |> 
  summarise(mean_crps = mean(skill_crps, na.rm = T)) |> 
  left_join(model_meta) |> 
  mutate(model_id = fct_reorder(model_id, mean_crps)) |> 
  ggplot(aes(x=mean_crps, y = model_id, fill = model_type)) +    # Different pattern for each group
  geom_bar_pattern(aes(pattern = as.factor(uses_NOAA)),
                   stat = "identity",
                   alpha = 0.8, 
                   pattern_fill = "black",
                   colour = "black", 
                   pattern_spacing = 0.02,
                   pattern_alpha = 0.8) + 
  scale_y_reordered() +
  labs(y='Model ID', x = 'mean CRPS skill (vs climatology)') +
  geom_vline(xintercept = 0) +
  scale_fill_manual(values = cols_modeltype) +
  scale_pattern_manual(values=c('none', 'stripe')) +
  scale_pattern_type_manual(values=c(NA, NA)) +
  theme_bw()

ggpubr::ggarrange(top10_temp, all_temperature, nrow = 1, 
                  common.legend = T,
                  legend = 'top', labels = 'auto')

```
 
9 of 10 top 10 models use covariates, and process models and ML are the most common best performers. 
There are a lot of really bad empirical models.
## Figure 2
How does model type performance, on avergae, differ by site. 

This uses all model_ids (maybe get rid of the 'worst') performers. 

```{r model_type-site}
temperature_skill |> 
  left_join(model_meta, by = 'model_id') |>
  filter(model_type != 'null') |> 
  group_by(model_type, model_id, site_id) |> 
  summarise(mean_skill = mean(skill_crps, na.rm = T)) |>  
  ggplot(aes(x = mean_skill, 
             y = site_id, 
             colour = model_type)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_wrap(~model_type, scales = 'free_y',nrow = 4) +
  labs(y='model_id', x = 'mean CRPS score') +
  scale_colour_manual(values = cols_modeltype) +
  theme_bw() 

temperature_climatology |> 
  group_by(site_id) |> 
  summarise(mean_score = mean(crps_climatology, na.rm = T))  |> 
  arrange(mean_score)
```

Gains at PRLA and PRPO relative to climatology (these have the worst climatology - see table), especially using process-models and ML.

The best climatology sites (BARC, SUGG) are difficult to beat even though perform well overall. 

## Figure 3
How does the performance vary across horizon? 
Do the models perform better consistently across the horizon at each site?
How do the best performing individual models perform?

```{r horizon-score}
# top 10 average models
top_mods <- temperature_skill |> 
  group_by(model_id) |> 
  summarise(mean_crps = mean(skill_crps, na.rm = T)) |> 
  ungroup() |> 
  slice_max(mean_crps, n = 10, na_rm = T) |> 
  distinct(model_id)

A <- temperature_skill |> 
  filter(model_id %in% top_mods$model_id) |> 
  group_by(site_id, horizon) |> 
  summarise(mean_crps = mean(skill_crps, na.rm = T)) |> 
  ggplot(aes(x=horizon, 
             y = mean_crps, colour = site_id)) +
  geom_hline(yintercept = 0) + 
  geom_line()  +  
  labs(y='mean CRPS skill (vs climatology)', x = 'horizon (days)') +
  scale_color_viridis_d(option = 'H') +
  theme_bw()

B <- temperature_skill |> 
  filter(model_id %in% top_mods$model_id) |> 
  group_by(model_id, horizon) |> 
  summarise(mean_crps = mean(skill_crps, na.rm = T)) |> 
  ggplot(aes(x=horizon, 
             y = mean_crps, colour = model_id)) +
  geom_hline(yintercept = 0) + 
  geom_line()  +  
  labs(y='mean CRPS skill (vs climatology)', x = 'horizon (days)') +
  scale_color_viridis_d(option = 'A',) +
  theme_bw()

ggpubr::ggarrange(A, B, labels = 'auto', align = 'v', nrow = 2)
```
Models beeating climatology at the shorter horizons (as expected) PRLA and PRPO always better than climatology.

Individual model performance decreases relative to climatology mostly except for random forest and lasso regression which improve after short horizons (may be related to the precision rather than accuracy). Less degradation in performance of ML models across the horizon (xgboost and random forest) than process models. 

## Figure 4 - NOAA scores

can this be related to the weather forecasts?

There is only one month when the observations overlap for the observational data...
But still the performance is better vs the observations than the stage 3

```{r}
NOAA_scores <- arrow::open_dataset("../scores") |>
  filter(site_id %in% lake_sites,
         model_id %in% c('NOAA_daily', 'NOAA_daily_obs')) |>
  collect()

NOAA_scores |> 
  mutate(horizon = as_date(datetime) - as_date(reference_datetime)) |> 
  select(site_id, crps, horizon, reference_datetime, model_id) |> 
  pivot_wider(names_from = site_id, values_from = crps, id_cols = horizon:model_id) |> 
  filter(reference_datetime > as_date('2023-05-25'),
         reference_datetime < as_date('2023-07-31')) |> 
  # only a 1 month overlap....
  pivot_longer(cols = BARC:TOOK, names_to = 'site_id', values_to = 'crps') |> 
  group_by(model_id, site_id, horizon) |> 
  summarise(mean_crps = mean(crps, na.rm = T)) |> 
  filter(horizon <= 30) |> 
  ggplot(aes(x=horizon, y=mean_crps, colour = model_id)) +
  geom_line() +
  facet_wrap(~site_id) +
  theme_bw() +
  labs(y = 'CRPS score (oC)', title = 'NOAA air temperature forecast')


```

## site differences in observations?

```{r obs-cv}
temperature_scores |> 
  mutate(horizon = as.numeric(as_date(datetime) - as_date(reference_datetime))) |> 
  filter(between(horizon, 0,30)) |> 
  group_by(site_id, variable, horizon) |> 
  summarise(cv = sd(observation, na.rm = T)/ mean(observation, na.rm = T)) |> 
  ggplot(aes(x=horizon, y= cv, colour = site_id)) +
  geom_point() +
  scale_colour_viridis_d(option = 'turbo') +
  theme_bw()
```


# Supplementary figures

```{r echo = F}
temperature_skill |> 
  left_join(model_meta, by = 'model_id') |>
  filter(model_type != 'null') |> 
  group_by(model_type, model_id, site_id) |> 
  summarise(mean_skill = mean(skill_crps, na.rm = T)) |>  
  ggplot(aes(x = mean_skill, 
             y = site_id, 
             colour = model_type)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_wrap(~model_type, scales = 'free_y',nrow = 4) +
  labs(y='model_id', x = 'mean CRPS score', title = 'all dates') +
  scale_colour_manual(values = cols_modeltype) +
  theme_bw() 

temperature_skill |> 
  left_join(model_meta, by = 'model_id') |>
  filter(model_type != 'null',
         reference_datetime > as_date('2023-06-28'),
         reference_datetime < as_date('2023-09-18')) |> 
  group_by(model_type, model_id, site_id) |> 
  summarise(mean_skill = mean(skill_crps, na.rm = T)) |>  
  ggplot(aes(x = mean_skill, 
             y = site_id, 
             colour = model_type)) +
  geom_vline(xintercept = 0, linetype = 'dashed') +
  geom_boxplot() +
  facet_wrap(~model_type, scales = 'free_y',nrow = 4) +
  labs(y='model_id', x = 'mean CRPS score', title = 'overlapping only') +
  scale_colour_manual(values = cols_modeltype) +
  theme_bw() 
```


